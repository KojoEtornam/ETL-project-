# ğŸŒ¦ï¸ Weather Data Engineering Pipelines (Airflow ETL)

This project contains a collection of **Weather ETL Pipelines** built with **Python** and orchestrated using **Apache Airflow**.  
It demonstrates real-world Data Engineering concepts including:

- API ingestion (OpenWeatherMap)
- ETL (Extract â†’ Transform â†’ Load)
- Airflow DAG scheduling
- SQLite data warehousing (local)
- Data normalization with pandas
- Modular, multi-pipeline architecture

This is one unified project containing **multiple pipelines**, each performing different ETL tasks.

---

# ğŸ“ Project Pipelines

### 1ï¸âƒ£ **Hourly Weather ETL (Accra)**
Fetches **current weather for Accra every hour**, transforms it, and stores it in SQLite.

**Use cases:** trend analysis, dashboards, monitoring.

### 2ï¸âƒ£ **Multi-City Weather ETL**
Fetches weather for **Accra, Kumasi, London, New York** every few hours and saves them together for comparison.

**Use cases:** cross-city analytics, multi-location monitoring.

### 3ï¸âƒ£ **Historical Weather ETL**
Loads historical weather data (API/CSV), cleans it, and stores it in SQLite for ML and forecasting.

**Use cases:** machine learning pipelines, forecasting, analytics, time-series modeling.

---

# ğŸ§± Tech Stack

| Component | Usage |
|----------|-------|
| **Python** | Core ETL logic |
| **Apache Airflow 2.x** | DAG scheduling & orchestration |
| **Requests** | API consumption |
| **Pandas** | Data transformations |
| **SQLite** | Local database (simple warehouse) |
| **Docker (optional)** | Airflow containerization |
| **Environment Variables** | Secure API key handling |

---

# ğŸ”§ Project Structure

```
weather-etl/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ multicity_weather_etl.py
â”‚   â”œâ”€â”€ weather_hourly_etl.py
â”‚   â””â”€â”€ historical_weather_etl.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ (SQLite databases stored here)       # Gitignored
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

# âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>
```

### 2ï¸âƒ£ Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate      # Linux / Mac
venv\Scripts\activate         # Windows
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set Environment Variables
Create a `.env` or export directly:

```bash
export OPENWEATHER_API_KEY="your_api_key"
export WEATHER_DB_PATH="./data/weather.db"
```

Never hardcode API keys.

---

# ğŸš€ Running the Pipelines

## **â–¶ï¸ Option A â€” Run in Airflow (Recommended)**

### Start Airflow locally:
```bash
airflow db init
airflow webserver -p 8080
airflow scheduler
```

Copy your DAGs into Airflow's `dags/` directory:

```
~/airflow/dags/
```

Then go to:

ğŸ”— **http://localhost:8080**

Enable and trigger your DAGs.

---

## **â–¶ï¸ Option B â€” Run ETL Logic Locally (Without Airflow)**

You can manually execute the pipelines for testing.

Example:
```python
from multicity_weather_etl import extract, transform, load

raw = extract()
clean = transform(data=raw)   # adjust signature for local testing
load(records=clean)
```

Useful for debugging transformation or DB issues.

---

# ğŸ—„ï¸ Database Schema

All ETL pipelines load into a `weather` table containing:

| Column | Description |
|--------|-------------|
| city | City name |
| temperature | Temperature (Â°C) |
| humidity | Humidity % |
| weather | Description text |
| wind_speed | Wind speed (m/s) |
| timestamp | UTC timestamp |

---

# ğŸ” Security Best Practices

- Store API keys in **environment variables**  
- Add `.env` to **.gitignore**  
- Do NOT store SQLite DB in GitHub  
- Use **PostgreSQL** / Snowflake / BigQuery for production workloads  
- Add Airflow secrets backend for sensitive credentials  

---

# ğŸ“ˆ Future Improvements (Data Engineering Roadmap)

| Area | Possible Upgrade |
|------|------------------|
| Storage | Move from SQLite â†’ PostgreSQL |
| Raw Zone | Add S3/GCS cloud storage bucket |
| Transformation | Add dbt models |
| Monitoring | Add Airflow alerts / Grafana |
| ML | Build predictive weather model |
| Tests | Add pytest for the transform step |

---

# ğŸ§ª Testing

You can add unit tests under:

```
tests/
```

Example test (pytest):
```python
def test_transform():
    sample = {...}
    output = transform(data=[sample])
    assert "temperature" in output[0]
```

---

# ğŸ“¦ Requirements

```
apache-airflow>=2.3.0
pandas>=1.4.0
requests>=2.28.0
python-dotenv>=0.21.0
```

---

# âœ¨ Author

**John Botsyoe Etornam**  
Data Engineering â€¢ Mobile Development â€¢ Cloud â€¢ Python  
GitHub: https://github.com/KojoEtornam  
LinkedIn: John (BOTSYOE) Etornam

---

